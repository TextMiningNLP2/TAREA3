{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAREA 3\n",
    "# NLP2\n",
    "# https://github.com/TextMiningNLP2/TAREA3/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SE IMPORTAN LAS LIBRERÍAS REQUERIDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SE IMPORTAN LAS LIBRERIAS INSTALAS\n",
    "# PREVIAMENTE SE HAN INSTALADO CIERTAS LIBERÍAS EN EL VENV\n",
    "\n",
    "import enum\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from typing import Generator, Dict, List, Tuple\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# De ser necesario, descargar en Corpora / Stop_words\n",
    "# Si se desea se puede descargar all_corpora pero toma tiempo\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "INICIALIZACIÓN DE FUNCIONES Y CLASES BÁSICAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASE DEFINIDORA DE SENTIMIENTOS\n",
    "# obtenida del ejemplo prepare_data\n",
    "\n",
    "class Sentiments(enum.Enum):\n",
    "    POS = 'POS'\n",
    "    NEG = 'NEG'\n",
    "\n",
    "# FUNCION PARA DIVIDIR DATA\n",
    "# obtenida del ejemplo train\n",
    "\n",
    "def split_data(data: List, weights: List = (0.7, 0.15, 0.15)):\n",
    "    split = {\n",
    "        'train': [],\n",
    "        'test': [],\n",
    "        'validation': [],\n",
    "    }\n",
    "    for word in data:\n",
    "        subset = random.choices(['train', 'test', 'validation'], weights=weights)[0]\n",
    "        split[subset].append(word)\n",
    "\n",
    "    return split\n",
    "\n",
    "# FUNCION CONVERTIR ORACIONES A PALABRAS\n",
    "# obtenida del ejemplo tokenize\n",
    "\n",
    "def sentences_to_words(sentences: List[str]) -> List[List[str]]:\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        # https://radimrehurek.com/gensim/utils.html#gensim.utils.simple_preprocess\n",
    "        words.append(simple_preprocess(str(sentence), deacc=True))  # deacc=True elimina la puntuación\n",
    "    return words\n",
    "\n",
    "# FUNCION QUE REMUEVE LOS STOPWORDS DADOS\n",
    "# obtenida del ejemplo tokenize\n",
    "\n",
    "def remove_stopwords(documents: List[List[str]]) -> List[List[str]]:\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stopwords.words('english')]\n",
    "            for doc in documents]\n",
    "\n",
    "# FUNCION PARA APRENDER BIGRAMAS\n",
    "# obtenida del ejemplo tokenize\n",
    "\n",
    "def learn_bigrams(documents: List[List[str]]) -> List[List[str]]:\n",
    "    # We learn bigrams\n",
    "    #  https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.Phrases\n",
    "    bigram = Phrases(documents, min_count=5, threshold=10)\n",
    "\n",
    "    # we reduce the bigram model to its minimal functionality\n",
    "    bigram_mod = Phraser(bigram)\n",
    "\n",
    "    # we apply the bigram model to our documents\n",
    "    return bigram_mod\n",
    "\n",
    "# FUNCION PARA CREAR BIGRAMAS\n",
    "# obtenida del ejemplo tokenize\n",
    "\n",
    "def create_bigrams(bigram_model, documents: List[List[str]]):\n",
    "    return [bigram_model[doc] for doc in documents]\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "LECTURA DEL ARCHIVO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw/reviews.csv')\n",
    "\n",
    "df['rating'] = df['rating'].astype(dtype='int64')\n",
    "df['sentiment'] = df['rating'].apply(lambda x: Sentiments.POS if x >= 40 else Sentiments.NEG)\n",
    "\n",
    "data_classes = {sentiment.value: df[df['sentiment'] == sentiment]['review'].values.tolist() for sentiment in Sentiments}\n",
    "\n",
    "pos_rev = data_classes['POS']\n",
    "neg_rev = data_classes['NEG']\n",
    "\n",
    "splitPOS = split_data(pos_rev)\n",
    "splitNEG = split_data(neg_rev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_data(pos_rev)['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(data_classes['NEG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARA GUARDAR ARCHIVO BINARIO DE PYTHON\n",
    "# NO ES NECESARIO CORRER, YA ESTA GUARDADA UNA COPIA\n",
    "\n",
    "with open(r\"../data/interim/review_classes.pkl\", \"wb\") as output_file:\n",
    "    pickle.dump(data_classes, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARA LEER ARCHIVO BINARIO DE PYTHON\n",
    "# CORRER ESTE PARA LEER LA COPIA GUARDADA\n",
    "\n",
    "with open(r\"../data/interim/review_classes.pkl\", \"rb\") as input_file:\n",
    "    review_classes = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SE DIVIDEN LAS CLASES EN POSITIVAS Y NEGATIVAS\n",
    "\n",
    "positive_reviews = review_classes['POS']\n",
    "negative_reviews = review_classes['NEG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SE DIVIDEN LAS CLASES EN PALABRAS UNICAS\n",
    "# NO ES NECESARIO CORRER\n",
    "# ABAJO LO TENEMOS PROCESADO, SOLO HACER ELCTURA\n",
    "\n",
    "# print(negative_reviews[1])\n",
    "negative_words = sentences_to_words(negative_reviews)\n",
    "positive_words = sentences_to_words(positive_reviews)\n",
    "# print(negative_words[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARA GUARDADO DE NEGATIVE/POSITIVE WORDS\n",
    "# NO ES NECESARIO CORRER, YA QUE SE ENTREGAN LOS ARCHIVOS YA GUARDADOS\n",
    "# SALTAR A LA SIGUIENTE CELDA\n",
    "with open(r\"../data/interim/negative_words.pkl\", \"wb\") as output_file:\n",
    "    pickle.dump(data_classes, output_file)\n",
    "    \n",
    "with open(r\"../data/interim/positive_words.pkl\", \"wb\") as output_file:\n",
    "    pickle.dump(data_classes, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARA LECTURA DE LOS NEGATIVE/POSITIVE WORDS YA PROCESADO\n",
    "# No se gasta tiempo en procesarlo otra vez\n",
    "\n",
    "with open(r\"../data/interim/review_classes.pkl\", \"rb\") as input_file:\n",
    "    negative_words = pickle.load(input_file)\n",
    "\n",
    "with open(r\"../data/interim/review_classes.pkl\", \"rb\") as input_file:\n",
    "    positive_words = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DE LAS VARIABLES ANTERIORES, SE ELIMINAN LOS STOPWORDS\n",
    "\n",
    "# print(negative_words[1])\n",
    "negative_words_wo_stopwords = remove_stopwords(negative_words)\n",
    "# print(negative_words_wo_stopwords[1])\n",
    "positive_words_wo_stopwords = remove_stopwords(positive_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SE CREA MODELO DE BIGRAMAS\n",
    "# A partir de TODO el corpus procesado\n",
    "bigram_model = learn_bigrams(negative_words_wo_stopwords + positive_words_wo_stopwords)\n",
    "negative_words_wo_stopwords_bigrams = create_bigrams(bigram_model, negative_words_wo_stopwords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
